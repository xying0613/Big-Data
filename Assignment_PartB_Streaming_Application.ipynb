{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Haw Xiao Ying <br>Student ID: 29797918 <br>Email: xhaw0001@student.monash.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygeohash as pgh\n",
    "\n",
    "def near_location(lat1, lon1, lat2, lon2):\n",
    "    location1 = pgh.encode(lat1, lon1, precision=3)\n",
    "    location2 = pgh.encode(lat2, lon2, precision=3)\n",
    "    if location1 == location2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_hash_precision_5(lat1, lon1):\n",
    "    location1 = pgh.encode(lat1, lon1, precision=5)\n",
    "    return(location1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "                \n",
    "                \n",
    "def sendDataToDB(iter):\n",
    "    \n",
    "    climate = []\n",
    "    aqua = []\n",
    "    terra = []\n",
    "    \n",
    "    for record in iter:\n",
    "        key = record[0]\n",
    "        if key == \"producer_1\":\n",
    "            climate.append(json.loads(record[1]))\n",
    "        elif key == \"producer_2\":\n",
    "            aqua.append(json.loads(record[1]))\n",
    "        else:\n",
    "            terra.append(json.loads(record[1]))\n",
    "            \n",
    "    \n",
    "    # If there is no climate data, then just end it\n",
    "    if len(climate) != 0:\n",
    "        \n",
    "        # Check if hotspot is near to climate\n",
    "        #  - Not close: ignore the hotspot data\n",
    "        #  - Close: store it into near_hotspot list for further usage\n",
    "        near_hotspot = []\n",
    "        for i in climate:\n",
    "            for j in aqua:\n",
    "                if (near_location(i['latitude'], i['longitude'], j['latitude'], j['longitude'])):\n",
    "                    near_hotspot.append(j)\n",
    "            for j in terra:\n",
    "                if (near_location(i['latitude'], i['longitude'], j['latitude'], j['longitude'])):\n",
    "                    near_hotspot.append(j)\n",
    "        \n",
    "        # If there are no near hotspots, then just store climate data only\n",
    "        if len(near_hotspot) != 0:\n",
    "            \n",
    "            # Get the hash of all hotspot\n",
    "            hotspot_hash = []\n",
    "            for location in near_hotspot:\n",
    "                hotspot_hash.append(get_hash_precision_5(location['latitude'], location['longitude']))\n",
    "            \n",
    "            # Check if there is same hash indicating same locations\n",
    "            temp = []\n",
    "            same_hotspot = []\n",
    "            for h in range(len(hotspot_hash)):\n",
    "                if hotspot_hash[h] not in temp:\n",
    "                    temp.append(hotspot_hash[h])\n",
    "                else:\n",
    "                    same_hotspot.append(h)\n",
    "                \n",
    "            # If there are hotspots with same location, merge them into 1\n",
    "            if len(same_hotspot) != 0:\n",
    "                temp = []\n",
    "                conf = []\n",
    "                lat = []\n",
    "                lon = []\n",
    "                same_hotspot = same_hotspot[::-1]\n",
    "                dic['time'] = near_hotspot[same_hotspot[0]]['time']\n",
    "                for index in same_hotspot:\n",
    "                    temp.append(near_hotspot[index]['surface_temperature_celcius'])\n",
    "                    conf.append(near_hotspot[index]['confidence'])\n",
    "                    lat.append(near_hotspot[index]['latitude'])\n",
    "                    lon.append(near_hotspot[index]['longitude'])\n",
    "                    producer = near_hotspot[index]['producer']\n",
    "                    near_hotspot.remove(near_hotspot[index])\n",
    "                avg_stc = round(sum(temp)/len(temp))\n",
    "                avg_c = round(sum(conf)/len(conf))\n",
    "                avg_lat = round(sum(lat)/len(lat))\n",
    "                avg_lon = round(sum(lon)/len(lon))\n",
    "\n",
    "                dic = {}\n",
    "                dic['surface_temperature_celcius'] = avg_stc\n",
    "                dic['confidence'] = avg_c\n",
    "                dic['latitude'] = avg_lat\n",
    "                dic['longitude'] = avg_lon\n",
    "                \n",
    "                near_hotspot.append(dic)\n",
    "                \n",
    "            # Check air temperature and GHI to determine cause of fire event\n",
    "            flag = False\n",
    "            if (climate[0]['air_temperature_celcius'] > 20 and climate[0]['GHI_w/m2'] > 180):\n",
    "                flag = True\n",
    "\n",
    "            # Now, for all hotspot data, follow date of climate data\n",
    "            date = climate[0]['date']\n",
    "            \n",
    "            for hotspot in near_hotspot:\n",
    "                hotspot['date'] = date\n",
    "                time = hotspot.pop('time')\n",
    "                hotspot['datetime'] = date + time\n",
    "                if flag:\n",
    "                    hotspot['cause'] = 'natural'\n",
    "                else:\n",
    "                    hotspot['cause'] = 'other'            \n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    client = MongoClient()\n",
    "    db = client.fit3182_assignment_db\n",
    "    try:\n",
    "        if len(climate) != 0:\n",
    "            db.climate_stream.insert(climate[0])\n",
    "            cursor = db.climate_stream.find({\"date\" : climate[0][\"date\"]})\n",
    "            for location in near_hotspot:\n",
    "                location[\"foreign_key\"] = cursor[0][\"_id\"]\n",
    "                db.hotspot_stream.insert(location)\n",
    "    except Exception as ex:\n",
    "        print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    client.close()\n",
    "\n",
    "\n",
    "    \n",
    "n_secs = 10\n",
    "topic = 'assignment'\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.fit3182_assignment_db\n",
    "db.climate_stream.drop()\n",
    "db.hotspot_stream.drop()\n",
    "\n",
    "client.close()\n",
    "\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "    \n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'week11-group', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
